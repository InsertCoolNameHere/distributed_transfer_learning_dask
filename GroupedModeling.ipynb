{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ordered-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from sklearn import ensemble\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-supervisor",
   "metadata": {},
   "source": [
    "<h3><u>GROUPING CLUSTERS FROM CSV</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moving-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters = pd.read_csv(\"/tmp/clusters_demo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "verbal-kruger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f4b99469f98>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clusters\n",
    "\n",
    "gk = df_clusters.groupby('cluster_id')\n",
    "\n",
    "gk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brave-waters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'G0800010': {'dist_min': 402.1741858065082, 'dist_max': 416.4204983057752, 'children': ['G1303070', 'G4701150', 'G1201210'], 'distances': [413.07465175307607, 409.1042783051329, 416.4204983057752]}, 'G3701430': {'dist_min': 410.7905596873448, 'dist_max': 412.12121359745873, 'children': ['G3701070', 'G4801550', 'G4001210', 'G0600090'], 'distances': [411.08717608030713, 412.12121359745873, 411.2011681070961, 411.6608777411628]}}\n",
      "{'G1303070': 'G0800010', 'G4701150': 'G0800010', 'G1201210': 'G0800010', 'G3701070': 'G3701430', 'G4801550': 'G3701430', 'G4001210': 'G3701430', 'G0600090': 'G3701430'}\n"
     ]
    }
   ],
   "source": [
    "parent_maps = {}\n",
    "child_to_parent = {}\n",
    "\n",
    "for name, group in gk:\n",
    "    row = group[group.distance == group.distance.min()]\n",
    "    row_max = group[group.distance == group.distance.max()]\n",
    "    \n",
    "    children = list(group.gis_join)\n",
    "    distances = list(group.distance)\n",
    "    \n",
    "    dist_min = row['distance'].item()\n",
    "    dist_max = row_max['distance'].item()\n",
    "    \n",
    "    pg = str(row['gis_join'].item())\n",
    "    \n",
    "    parent_index = children.index(pg)\n",
    "    children.pop(parent_index)\n",
    "    distances.pop(parent_index)\n",
    "    \n",
    "    inner_dict = {}\n",
    "    inner_dict['dist_min'] = dist_min\n",
    "    inner_dict['dist_max'] = dist_max\n",
    "    inner_dict['children'] = children\n",
    "    inner_dict['distances'] = distances\n",
    "    \n",
    "    parent_maps[pg] = inner_dict\n",
    "    \n",
    "    for c in children:\n",
    "        child_to_parent[c] = pg\n",
    "    \n",
    "                           \n",
    "print(parent_maps)\n",
    "print(child_to_parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-michigan",
   "metadata": {},
   "source": [
    "<h3><u>CONSTANTS AND HELPER FUNCTIONS</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "material-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_min = 0.05\n",
    "sample_max = 0.25\n",
    "\n",
    "query_collection = \"macav2\"\n",
    "mongo_url = \"mongodb://lattice-100:27018/\"\n",
    "mongo_db_name = \"sustaindb\"\n",
    "query_fild = \"gis_join\"\n",
    "train_test = 0.8\n",
    "\n",
    "\n",
    "training_labels = [\"min_surface_downwelling_shortwave_flux_in_air\", \"max_surface_downwelling_shortwave_flux_in_air\",\n",
    "                   \"max_specific_humidity\", \"min_max_air_temperature\", \"max_max_air_temperature\"]\n",
    "target_labels = [\"max_min_air_temperature\"]\n",
    "\n",
    "\n",
    "# QUERY projection\n",
    "client_projection = {}\n",
    "for val in training_labels:\n",
    "    client_projection[val] = 1\n",
    "for val in target_labels:\n",
    "    client_projection[val] = 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-madagascar",
   "metadata": {},
   "source": [
    "<h1><u>MODELING</u></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stunning-trace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saved_models = {}\n",
    "\n",
    "# ACTUAL QUERYING\n",
    "def query_sustaindb(query_gisjoin, sustain_db):\n",
    "    sustain_collection = sustain_db[query_collection]\n",
    "    client_query = {query_fild: query_gisjoin}\n",
    "    query_results = list(sustain_collection.find(client_query, client_projection)) \n",
    "    return list(query_results)\n",
    "\n",
    "# SAMPLE FROM QUERY RESULTS\n",
    "def data_sampling(query_results, exhaustive, sample_percent=1):\n",
    "    if exhaustive:\n",
    "        all_data = query_results\n",
    "    else:\n",
    "        data_size = int(len(query_results) * sample_percent)\n",
    "        all_data = sample(query_results, data_size)\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "# GET SAMPLE % BASED ON DISTANCE FROM CENTROID\n",
    "def get_sample_percent(gis_join):\n",
    "    parent_gis = child_to_parent[gis_join]\n",
    "    inner_dict = parent_maps[parent_gis]\n",
    "    d_max = inner_dict['dist_max']\n",
    "    d_min = inner_dict['dist_min']\n",
    "    children = inner_dict['children']\n",
    "    distances = inner_dict['distances']\n",
    "    \n",
    "    my_index = children.index(gis_join)\n",
    "    my_distance = distances[my_index]\n",
    "    \n",
    "    frac = (my_distance - d_min)/(d_max - d_min)\n",
    "    \n",
    "    perc = sample_min + (sample_max - sample_min) * frac\n",
    "    \n",
    "    perc*=100\n",
    "    perc = int(perc)\n",
    "    perc = perc - (perc%5)\n",
    "    \n",
    "    perc = perc/100\n",
    "    return perc\n",
    "\n",
    "def exhaustive_training(X,Y, gis_join):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    \n",
    "    param_grid = {'max_depth': [2, 3], 'min_samples_split': [15, 20, 50]}\n",
    "    base_est = ensemble.RandomForestRegressor(random_state=0)\n",
    "    sh = HalvingGridSearchCV(base_est, param_grid, cv=5, verbose=1, \n",
    "                             factor=2, resource='n_estimators', max_resources=600).fit(X, pd.Series.ravel(Y))\n",
    "    \n",
    "    clf_best = sh.best_estimator_\n",
    "    rmse = sqrt(mean_squared_error(pd.Series.ravel(y_test), clf_best.predict(X_test)))\n",
    "    \n",
    "    print(\"PARENT GISJOIN: \",gis_join, \"RMSE:\", rmse)\n",
    "    return clf_best\n",
    "    \n",
    "\n",
    "def sampled_training(X, Y, gis_join, saved_models):\n",
    "    parent_gis = child_to_parent[gis_join]\n",
    "    clf = saved_models[parent_gis]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    \n",
    "    clf.fit(X_train, pd.Series.ravel(y_train))\n",
    "\n",
    "    rmse = sqrt(mean_squared_error(pd.Series.ravel(y_test), clf.predict(X_test)))\n",
    "    \n",
    "    print(\"CHILD GISJOIN: \",gis_join, \"RMSE:\", rmse)\n",
    "    return clf\n",
    "    \n",
    "\n",
    "def train_gisjoin(gis_join, exhaustive=True, saved_models={}):\n",
    "    sustainclient = pymongo.MongoClient(mongo_url)\n",
    "    sustain_db = sustainclient[mongo_db_name]\n",
    "\n",
    "    sample_percent = 1\n",
    "    if not exhaustive:\n",
    "        #print(\"SAMPLED CHILD TRAINING.....\")\n",
    "        sample_percent = get_sample_percent(gis_join)\n",
    "        #print(\"SAMPLE PERCENT: \", sample_percent)\n",
    "        \n",
    "    #QUERY\n",
    "    results = query_sustaindb(gis_join, sustain_db)\n",
    "    \n",
    "    df_sampled = data_sampling(results, exhaustive, sample_percent)\n",
    "    \n",
    "    Y = df_sampled.loc[:,target_labels]\n",
    "    X = df_sampled.loc[:, training_labels]\n",
    "    #print(X.shape, Y.shape)\n",
    "    \n",
    "    if exhaustive:\n",
    "        clf = exhaustive_training(X,Y, gis_join)\n",
    "    else:\n",
    "        clf = sampled_training(X,Y, gis_join, saved_models)\n",
    "    \n",
    "    #saved_models[gis_join] = clf\n",
    "    return (gis_join,clf)\n",
    "    \n",
    "#'G1303070': 'G0800010'\n",
    "#train_gisjoin('G0800010', True)\n",
    "#train_gisjoin('G1303070', False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "editorial-official",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/parsons/b/others/sustain/.local/lib/python3.6/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 35357 instead\n",
      "  http_address[\"port\"], self.http_server.port\n"
     ]
    }
   ],
   "source": [
    "from dask import delayed\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "blocked-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "outputs = []\n",
    "\n",
    "# TRAINING PARENTS FIRST\n",
    "for pk in parent_maps.keys():\n",
    "    ret = delayed(train_gisjoin)(pk,True)\n",
    "    outputs.append(ret)\n",
    "\n",
    "futures = dask.persist(*outputs)  # trigger computation in the background\n",
    "results = dask.compute(*futures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "funded-parliament",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('G0800010', RandomForestRegressor(max_depth=3, min_samples_split=20, n_estimators=600,\n",
      "                      random_state=0)), ('G3701430', RandomForestRegressor(max_depth=3, min_samples_split=20, n_estimators=600,\n",
      "                      random_state=0)))\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "configured-battery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'G0800010': RandomForestRegressor(max_depth=3, min_samples_split=20, n_estimators=600,\n",
      "                      random_state=0), 'G3701430': RandomForestRegressor(max_depth=3, min_samples_split=20, n_estimators=600,\n",
      "                      random_state=0)}\n"
     ]
    }
   ],
   "source": [
    "for sm in results:\n",
    "    (gis_join, model) = sm\n",
    "    saved_models[gis_join] = model\n",
    "\n",
    "print(saved_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "distributed-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2 = []\n",
    "\n",
    "# TRAINING CHILDREN NEXT\n",
    "for ck in child_to_parent.keys():\n",
    "    ret = delayed(train_gisjoin)(ck,False, saved_models)\n",
    "    outputs2.append(ret)\n",
    "\n",
    "futures2 = dask.persist(*outputs2)  # trigger computation in the background\n",
    "results2 = dask.compute(*futures2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "comprehensive-terminology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'G0800010': RandomForestRegressor(max_depth=3, min_samples_split=20, n_estimators=600,\n",
      "                      random_state=0), 'G3701430': RandomForestRegressor(max_depth=3, min_samples_split=20, n_estimators=600,\n",
      "                      random_state=0), 'G1303070': RandomForestRegressor(max_depth=3, min_samples_split=20, n_estimators=600,\n",
      "                      random_state=0), 'G4701150': RandomForestRegressor(max_depth=3, min_samples_split=20, n_estimators=600,\n",
      "                      random_state=0), 'G1201210': RandomForestRegressor(max_depth=3, min_samples_split=20, n_estimators=600,\n",
      "                      random_state=0), 'G3701070': RandomForestRegressor(max_depth=3, min_samples_split=20, n_estimators=600,\n",
      "                      random_state=0), 'G4801550': RandomForestRegressor(max_depth=3, min_samples_split=20, n_estimators=600,\n",
      "                      random_state=0), 'G4001210': RandomForestRegressor(max_depth=3, min_samples_split=20, n_estimators=600,\n",
      "                      random_state=0), 'G0600090': RandomForestRegressor(max_depth=3, min_samples_split=20, n_estimators=600,\n",
      "                      random_state=0)}\n"
     ]
    }
   ],
   "source": [
    "for sm in results2:\n",
    "    (gis_join, model) = sm\n",
    "    saved_models[gis_join] = model\n",
    "\n",
    "print(saved_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-professional",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
