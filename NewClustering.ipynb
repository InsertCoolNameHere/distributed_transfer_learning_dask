{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "civilian-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from sklearn import ensemble\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-engagement",
   "metadata": {},
   "source": [
    "<h3><u>CONSTANTS AND HELPER FUNSTIONS</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "beneficial-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_collection = \"macav2\"\n",
    "mongo_url = \"mongodb://lattice-100:27018/\"\n",
    "mongo_db_name = \"sustaindb\"\n",
    "query_fild = \"gis_join\"\n",
    "sample_percent = 0.1\n",
    "train_test = 0.8\n",
    "feature_importance_percentage = 98\n",
    "exhaustive_sample_percent = 0.0001\n",
    "\n",
    "\n",
    "training_labels = [\"min_surface_downwelling_shortwave_flux_in_air\", \"max_surface_downwelling_shortwave_flux_in_air\",\n",
    "                   \"max_specific_humidity\", \"min_max_air_temperature\", \"max_max_air_temperature\"]\n",
    "target_labels = [\"max_min_air_temperature\"]\n",
    "\n",
    "# QUERY-RELATED\n",
    "sustainclient = pymongo.MongoClient(mongo_url)\n",
    "sustain_db = sustainclient[mongo_db_name]\n",
    "\n",
    "# QUERY projection\n",
    "client_projection = {}\n",
    "for val in training_labels:\n",
    "    client_projection[val] = 1\n",
    "for val in target_labels:\n",
    "    client_projection[val] = 1\n",
    "    \n",
    "    \n",
    "def fancy_logging(msg, unique_id=\"\"):\n",
    "    print(unique_id, \":\", \"====================================\")\n",
    "    print(unique_id, \":\", msg, \": TIME: \",time())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-pharmacy",
   "metadata": {},
   "source": [
    "<h3><u>DATA FETCH</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "controlled-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL QUERYING\n",
    "def query_sustaindb(query_gisjoin):\n",
    "\n",
    "    sustain_collection = sustain_db[query_collection]\n",
    "    client_query = {query_fild: query_gisjoin}\n",
    "\n",
    "    start_time = time()\n",
    "    query_results = list(sustain_collection.find(client_query, client_projection))\n",
    "    \n",
    "    return list(query_results)\n",
    "\n",
    "def queryall_sustaindb():\n",
    "\n",
    "    sustain_collection = sustain_db[query_collection]\n",
    "    client_query = {}\n",
    "\n",
    "    start_time = time()\n",
    "    query_results = list(sustain_collection.find(client_query, client_projection))\n",
    "    \n",
    "    return list(query_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "metropolitan-assignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  11353524\n"
     ]
    }
   ],
   "source": [
    "#df = query_sustaindb('G3701310')\n",
    "df = queryall_sustaindb()\n",
    "print(\"1: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-receptor",
   "metadata": {},
   "source": [
    "<h3><u>DATA SAMPLING</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "built-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sampling(query_results, exhaustive, sample_percent=1):\n",
    "    if exhaustive:\n",
    "        all_data = query_results\n",
    "    else:\n",
    "        data_size = int(len(query_results) * sample_percent)\n",
    "        all_data = sample(query_results, data_size)\n",
    "\n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mysterious-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIKI\n",
    "sampled_df = data_sampling(df, False, exhaustive_sample_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "liberal-trouble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1135, 5) (1135, 1)\n"
     ]
    }
   ],
   "source": [
    "Y = sampled_df.loc[:,target_labels]\n",
    "X = sampled_df.loc[:, training_labels]\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-parallel",
   "metadata": {},
   "source": [
    "<h3><u>DATA SPLITTING INTO TRAING AND VALIDATION</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "differential-porter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def data_partitioning(query_results, exhaustive, sample_percent=1):\\n    if exhaustive:\\n        all_data = query_results\\n    else:\\n        data_size = int(len(query_results) * sample_percent)\\n        all_data = sample(query_results, data_size)\\n\\n    msk = np.random.rand(len(all_data)) < train_test_split\\n\\n    all_data = pd.DataFrame(all_data)\\n    training_data = all_data[msk]\\n    val_data = all_data[~msk]\\n    return (pd.DataFrame(training_data), pd.DataFrame(val_data))'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def data_partitioning(query_results, exhaustive, sample_percent=1):\n",
    "    if exhaustive:\n",
    "        all_data = query_results\n",
    "    else:\n",
    "        data_size = int(len(query_results) * sample_percent)\n",
    "        all_data = sample(query_results, data_size)\n",
    "\n",
    "    msk = np.random.rand(len(all_data)) < train_test\n",
    "\n",
    "    all_data = pd.DataFrame(all_data)\n",
    "    training_data = all_data[msk]\n",
    "    val_data = all_data[~msk]\n",
    "    return (pd.DataFrame(training_data), pd.DataFrame(val_data))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "alien-geometry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(908, 5) (227, 5) (908, 1) (227, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-community",
   "metadata": {},
   "source": [
    "<h3><u>MODELING</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''parameters = [ {'n_estimators': 500, 'max_depth': 2, 'min_samples_split': 20},\n",
    "                {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 50},\n",
    "              {'n_estimators': 500, 'max_depth': 3, 'min_samples_split': 20},\n",
    "              {'n_estimators': 600, 'max_depth': 3, 'min_samples_split': 15}]\n",
    "    \n",
    "for params in parameters:\n",
    "    print(\"PARAMETERS:\",params)\n",
    "    count = 0\n",
    "    error = 0\n",
    "    for i in range(0,5):\n",
    "        print(\"ROUND:\",i)\n",
    "        clf = ensemble.RandomForestRegressor(**params)\n",
    "        clf.fit(X_train, pd.Series.ravel(y_train))\n",
    "\n",
    "        rmse = sqrt(mean_squared_error(pd.Series.ravel(y_test), clf.predict(X_test)))\n",
    "        print(rmse)\n",
    "        error = error + rmse\n",
    "\n",
    "        feature_importance = clf.feature_importances_\n",
    "        feature_importance = 100.0 * (feature_importance / feature_importance.sum())\n",
    "        sorted_idx = np.argsort(feature_importance)\n",
    "        count = count+1\n",
    "        print(np.flip(sorted_idx), np.flip(feature_importance[sorted_idx]))\n",
    "    \n",
    "    print(\"===============================================================\")\n",
    "\n",
    "    print(\"AVG RMSE:\",(error/count))'''\n",
    "\n",
    "# BETTER ALTERNATIVE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abstract-thousand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 150\n",
      "max_resources_: 600\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 6\n",
      "n_resources: 150\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV 1/5] END max_depth=2, min_samples_split=15, n_estimators=150;, score=(train=0.880, test=0.845) total time=   0.4s\n",
      "[CV 2/5] END max_depth=2, min_samples_split=15, n_estimators=150;, score=(train=0.884, test=0.874) total time=   0.3s\n",
      "[CV 3/5] END max_depth=2, min_samples_split=15, n_estimators=150;, score=(train=0.880, test=0.876) total time=   0.6s\n",
      "[CV 4/5] END max_depth=2, min_samples_split=15, n_estimators=150;, score=(train=0.881, test=0.901) total time=   0.2s\n",
      "[CV 5/5] END max_depth=2, min_samples_split=15, n_estimators=150;, score=(train=0.869, test=0.866) total time=   0.2s\n",
      "[CV 1/5] END max_depth=2, min_samples_split=20, n_estimators=150;, score=(train=0.880, test=0.845) total time=   0.2s\n",
      "[CV 2/5] END max_depth=2, min_samples_split=20, n_estimators=150;, score=(train=0.884, test=0.874) total time=   0.2s\n",
      "[CV 3/5] END max_depth=2, min_samples_split=20, n_estimators=150;, score=(train=0.880, test=0.876) total time=   0.2s\n",
      "[CV 4/5] END max_depth=2, min_samples_split=20, n_estimators=150;, score=(train=0.881, test=0.901) total time=   0.2s\n",
      "[CV 5/5] END max_depth=2, min_samples_split=20, n_estimators=150;, score=(train=0.869, test=0.866) total time=   0.2s\n",
      "[CV 1/5] END max_depth=2, min_samples_split=50, n_estimators=150;, score=(train=0.880, test=0.845) total time=   0.2s\n",
      "[CV 2/5] END max_depth=2, min_samples_split=50, n_estimators=150;, score=(train=0.884, test=0.874) total time=   0.2s\n",
      "[CV 3/5] END max_depth=2, min_samples_split=50, n_estimators=150;, score=(train=0.880, test=0.876) total time=   0.2s\n",
      "[CV 4/5] END max_depth=2, min_samples_split=50, n_estimators=150;, score=(train=0.881, test=0.901) total time=   0.2s\n",
      "[CV 5/5] END max_depth=2, min_samples_split=50, n_estimators=150;, score=(train=0.869, test=0.866) total time=   0.2s\n",
      "[CV 1/5] END max_depth=3, min_samples_split=15, n_estimators=150;, score=(train=0.931, test=0.913) total time=   0.3s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=15, n_estimators=150;, score=(train=0.932, test=0.914) total time=   0.3s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=15, n_estimators=150;, score=(train=0.931, test=0.924) total time=   0.3s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=15, n_estimators=150;, score=(train=0.930, test=0.931) total time=   0.3s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=15, n_estimators=150;, score=(train=0.927, test=0.927) total time=   0.3s\n",
      "[CV 1/5] END max_depth=3, min_samples_split=20, n_estimators=150;, score=(train=0.931, test=0.913) total time=   0.3s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=20, n_estimators=150;, score=(train=0.932, test=0.913) total time=   0.3s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=20, n_estimators=150;, score=(train=0.931, test=0.924) total time=   0.3s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=20, n_estimators=150;, score=(train=0.930, test=0.931) total time=   0.3s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=20, n_estimators=150;, score=(train=0.927, test=0.927) total time=   0.3s\n",
      "[CV 1/5] END max_depth=3, min_samples_split=50, n_estimators=150;, score=(train=0.930, test=0.912) total time=   0.3s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=50, n_estimators=150;, score=(train=0.931, test=0.912) total time=   0.3s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=50, n_estimators=150;, score=(train=0.931, test=0.924) total time=   0.3s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=50, n_estimators=150;, score=(train=0.927, test=0.930) total time=   0.3s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=50, n_estimators=150;, score=(train=0.927, test=0.927) total time=   0.3s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 3\n",
      "n_resources: 300\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV 1/5] END max_depth=3, min_samples_split=50, n_estimators=300;, score=(train=0.930, test=0.913) total time=   0.5s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=50, n_estimators=300;, score=(train=0.931, test=0.912) total time=   0.5s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=50, n_estimators=300;, score=(train=0.930, test=0.923) total time=   0.5s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=50, n_estimators=300;, score=(train=0.927, test=0.931) total time=   0.5s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=50, n_estimators=300;, score=(train=0.926, test=0.927) total time=   0.5s\n",
      "[CV 1/5] END max_depth=3, min_samples_split=20, n_estimators=300;, score=(train=0.931, test=0.914) total time=   0.5s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=20, n_estimators=300;, score=(train=0.932, test=0.913) total time=   0.5s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=20, n_estimators=300;, score=(train=0.931, test=0.923) total time=   0.5s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=20, n_estimators=300;, score=(train=0.929, test=0.931) total time=   0.5s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=20, n_estimators=300;, score=(train=0.926, test=0.927) total time=   0.5s\n",
      "[CV 1/5] END max_depth=3, min_samples_split=15, n_estimators=300;, score=(train=0.931, test=0.914) total time=   0.5s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=15, n_estimators=300;, score=(train=0.932, test=0.913) total time=   0.5s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=15, n_estimators=300;, score=(train=0.931, test=0.923) total time=   0.6s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=15, n_estimators=300;, score=(train=0.929, test=0.931) total time=   0.5s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=15, n_estimators=300;, score=(train=0.926, test=0.927) total time=   0.5s\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 600\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END max_depth=3, min_samples_split=20, n_estimators=600;, score=(train=0.931, test=0.914) total time=   1.0s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=20, n_estimators=600;, score=(train=0.932, test=0.914) total time=   1.0s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=20, n_estimators=600;, score=(train=0.931, test=0.923) total time=   1.0s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=20, n_estimators=600;, score=(train=0.930, test=0.932) total time=   1.0s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=20, n_estimators=600;, score=(train=0.926, test=0.927) total time=   1.0s\n",
      "[CV 1/5] END max_depth=3, min_samples_split=15, n_estimators=600;, score=(train=0.931, test=0.914) total time=   1.0s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=15, n_estimators=600;, score=(train=0.932, test=0.914) total time=   1.0s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=15, n_estimators=600;, score=(train=0.931, test=0.923) total time=   1.0s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=15, n_estimators=600;, score=(train=0.930, test=0.932) total time=   1.0s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=15, n_estimators=600;, score=(train=0.926, test=0.927) total time=   1.0s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "param_grid = {'max_depth': [2, 3], 'min_samples_split': [15, 20, 50]}\n",
    "base_est = ensemble.RandomForestRegressor(random_state=0)\n",
    "sh = HalvingGridSearchCV(base_est, param_grid, cv=5, verbose=3, \n",
    "                         factor=2, resource='n_estimators', max_resources=600).fit(X, pd.Series.ravel(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "agricultural-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE BEST MODEL\n",
    "clf_best = sh.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "settled-warning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3194087807191863"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = sqrt(mean_squared_error(pd.Series.ravel(y_test), clf_best.predict(X_test)))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-sarah",
   "metadata": {},
   "source": [
    "<h3><u>EXTRACT TOP FEATURES</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "diagnostic-device",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 3 0 1] [9.54214074e+01 4.38867615e+00 1.88880199e-01 1.03620492e-03\n",
      " 0.00000000e+00]\n",
      "[2 4 3 0 1]\n",
      "[9.54214074e+01 4.38867615e+00 1.88880199e-01 1.03620492e-03\n",
      " 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "feature_importance = clf_best.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.sum())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "print(np.flip(sorted_idx), np.flip(feature_importance[sorted_idx]))\n",
    "\n",
    "feature_importance = np.flip(feature_importance[sorted_idx])\n",
    "sorted_idx=np.flip(sorted_idx)\n",
    "\n",
    "print(sorted_idx)\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-israeli",
   "metadata": {},
   "source": [
    "<h3><u>FIND N FOR WHICH IMPORTANCE % > feature-importance-percentage</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "apparent-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cumulative(lists, val_max):\n",
    "    cu_list = []\n",
    "    length = len(lists)\n",
    "    cu_list = [sum(lists[0:x:1]) for x in range(1, length+1)]\n",
    "    \n",
    "    print(cu_list)\n",
    "    res = next(x for x, val in enumerate(cu_list)\n",
    "                                  if val > val_max)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "convenient-footage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95.4214074454134, 99.8100835960911, 99.9989637950784, 100.0, 100.0]\n",
      "LAST INDEX:  1\n"
     ]
    }
   ],
   "source": [
    "cut_off_indx = find_cumulative(feature_importance, feature_importance_percentage)\n",
    "\n",
    "print(\"LAST INDEX: \", cut_off_indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "third-handling",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 3 0 1]\n",
      "[2 4]\n"
     ]
    }
   ],
   "source": [
    "chopped_indices = sorted_idx[0:cut_off_indx+1]\n",
    "\n",
    "print(sorted_idx)\n",
    "print(chopped_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-profit",
   "metadata": {},
   "source": [
    "<h3><u>SELECTED TOP COLUMNS</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "expressed-motor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['min_surface_downwelling_shortwave_flux_in_air', 'max_surface_downwelling_shortwave_flux_in_air', 'max_specific_humidity', 'min_max_air_temperature', 'max_max_air_temperature']\n",
      "['max_min_air_temperature']\n"
     ]
    }
   ],
   "source": [
    "candidate_x_columns = list(X.columns)\n",
    "candidate_y_columns = list(Y.columns)\n",
    "\n",
    "print(candidate_x_columns)\n",
    "print(candidate_y_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "political-sight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['max_specific_humidity', 'max_max_air_temperature']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_x_columns = [candidate_x_columns[i] for i in chopped_indices]\n",
    "selected_x_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-topic",
   "metadata": {},
   "source": [
    "<b><hr /></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-strand",
   "metadata": {},
   "source": [
    "<h1><u><b>TRAINING PHASE #2</b></u></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-boston",
   "metadata": {},
   "source": [
    "<h3><u>AGGREGATE QUERY OVER THE CHOSEN COLUMNS PER GIS-JOIN</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fitting-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sustain_collection = sustain_db[query_collection]\n",
    "pipeline=[\n",
    "   { \"$project\": { 'gis_join': '$gis_join', 'max_specific_humidity': '$max_specific_humidity', 'max_max_air_temperature': '$max_max_air_temperature', 'max_min_air_temperature': '$max_min_air_temperature'}},\n",
    "   { \"$group\": { '_id': \"$gis_join\", \n",
    "\"avg_max_specific_humidity\": { \"$avg\": \"$max_specific_humidity\" },\n",
    "\"avg_max_max_air_temperature\": { \"$avg\": \"$max_max_air_temperature\" },\n",
    "\"avg_max_min_air_temperature\": { \"$avg\": \"$max_min_air_temperature\" }\n",
    "  } }\n",
    "]\n",
    "cur = sustain_collection.aggregate(pipeline)\n",
    "\n",
    "results = list(cur)\n",
    "len(results)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "known-situation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['max_specific_humidity', 'max_max_air_temperature', 'max_min_air_temperature']\n"
     ]
    }
   ],
   "source": [
    "chopped_projection = []\n",
    "chopped_projection.extend(selected_x_columns)\n",
    "chopped_projection.extend(candidate_y_columns)\n",
    "\n",
    "print(chopped_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "happy-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_chopped_query(chopped_projection, gis_join):\n",
    "    # PROJECTION\n",
    "    proj_d = {}\n",
    "    proj_dict = {'$project': proj_d}\n",
    "    \n",
    "    #GROUP + AGGREGATION\n",
    "    group_d = {}\n",
    "    group_dict = {'$group': group_d}\n",
    "    \n",
    "    full_query=[proj_dict, group_dict]\n",
    "    \n",
    "    # PROJECTION PART\n",
    "    for cp in chopped_projection:\n",
    "        proj_d[cp] = \"$\"+str(cp)\n",
    "    proj_d[gis_join] = \"$\"+str(gis_join)\n",
    "    \n",
    "    # GROUP PART\n",
    "    group_d['_id'] = \"$\"+str(gis_join)\n",
    "    for cp in chopped_projection:\n",
    "        inner_dict = {}\n",
    "        inner_dict[\"$avg\"] = \"$\"+str(cp)\n",
    "        group_d[cp] = inner_dict\n",
    "    \n",
    "    return full_query\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "inclusive-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_pipeline = construct_chopped_query(chopped_projection, query_fild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "recreational-visiting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3108\n"
     ]
    }
   ],
   "source": [
    "cur = sustain_collection.aggregate(agg_pipeline)\n",
    "agg_results = list(cur)\n",
    "\n",
    "print(len(agg_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "promising-rocket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': 'G1303070',\n",
       " 'max_specific_humidity': 0.010248015329865863,\n",
       " 'max_max_air_temperature': 298.66463618943334,\n",
       " 'max_min_air_temperature': 285.35960279222553}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-water",
   "metadata": {},
   "source": [
    "<h3><u>DATA STAGING FOR PHASE 2</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "advisory-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase2_df = pd.DataFrame(agg_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "suburban-bidding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['max_specific_humidity', 'max_max_air_temperature', 'max_min_air_temperature']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chopped_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "finnish-spank",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_specific_humidity</th>\n",
       "      <th>max_max_air_temperature</th>\n",
       "      <th>max_min_air_temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010248</td>\n",
       "      <td>298.664636</td>\n",
       "      <td>285.359603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009536</td>\n",
       "      <td>295.464063</td>\n",
       "      <td>282.961655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011345</td>\n",
       "      <td>301.200444</td>\n",
       "      <td>287.548820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005304</td>\n",
       "      <td>292.316465</td>\n",
       "      <td>276.215785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009168</td>\n",
       "      <td>297.007123</td>\n",
       "      <td>284.217233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3103</th>\n",
       "      <td>0.008729</td>\n",
       "      <td>295.586985</td>\n",
       "      <td>283.527293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3104</th>\n",
       "      <td>0.008122</td>\n",
       "      <td>295.232022</td>\n",
       "      <td>281.684612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105</th>\n",
       "      <td>0.006237</td>\n",
       "      <td>287.522124</td>\n",
       "      <td>276.567473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3106</th>\n",
       "      <td>0.006625</td>\n",
       "      <td>290.181618</td>\n",
       "      <td>280.437066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>0.007102</td>\n",
       "      <td>290.849499</td>\n",
       "      <td>279.030655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3108 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      max_specific_humidity  max_max_air_temperature  max_min_air_temperature\n",
       "0                  0.010248               298.664636               285.359603\n",
       "1                  0.009536               295.464063               282.961655\n",
       "2                  0.011345               301.200444               287.548820\n",
       "3                  0.005304               292.316465               276.215785\n",
       "4                  0.009168               297.007123               284.217233\n",
       "...                     ...                      ...                      ...\n",
       "3103               0.008729               295.586985               283.527293\n",
       "3104               0.008122               295.232022               281.684612\n",
       "3105               0.006237               287.522124               276.567473\n",
       "3106               0.006625               290.181618               280.437066\n",
       "3107               0.007102               290.849499               279.030655\n",
       "\n",
       "[3108 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_importance = phase2_df.loc[:, chopped_projection]\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-bullet",
   "metadata": {},
   "source": [
    "<h3><u>K-MEANS CLUSTERING</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bridal-cartridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = int(sqrt(len(agg_results)))\n",
    "\n",
    "num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "superior-savage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016077470572132494\n",
      "306.7446375581714\n",
      "295.760583082398\n"
     ]
    }
   ],
   "source": [
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "copyrighted-behavior",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-3.40282347e+38 -3.40282347e+38 -3.40282347e+38]\n",
      " [-1.84467441e+20 -1.84467441e+20 -1.84467441e+20]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/parsons/b/others/sustain/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (55). Possibly due to duplicate points in X.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=num_clusters).fit(df_importance)\n",
    "centroids = kmeans.cluster_centers_\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "comparable-young",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      gis_join cluster_id    distance\n",
      "0     G1303070          0  413.074652\n",
      "1     G4701150          0  409.104278\n",
      "2     G1201210          0  416.420498\n",
      "3     G0800010          0  402.174186\n",
      "4     G3701070          0  411.087176\n",
      "...        ...        ...         ...\n",
      "3103  G0500750          0  409.584414\n",
      "3104  G0500490          0  408.054123\n",
      "3105  G5500390          0  398.946787\n",
      "3106  G5300570          0  403.547171\n",
      "3107  G3901350          0  403.052772\n",
      "\n",
      "[3108 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "df_ultimate = pd.DataFrame(columns=[\"gis_join\", \"cluster_id\", \"distance\"])\n",
    "\n",
    "\n",
    "\n",
    "for index, row in phase2_df.iterrows():\n",
    "    input_x = row[chopped_projection]\n",
    "    gis_join = row['_id']\n",
    "    #print(input_x, gis_join)\n",
    "    closest, d = pairwise_distances_argmin_min([np.array(input_x)], centroids)\n",
    "    df_ultimate.loc[index] = [gis_join, closest[0], d[0]]\n",
    "    \n",
    "print(df_ultimate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "likely-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultimate.to_csv(\"/tmp/clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-partnership",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
