{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "civilian-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from sklearn import ensemble\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-engagement",
   "metadata": {},
   "source": [
    "<h3><u>CONSTANTS AND HELPER FUNSTIONS</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beneficial-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_collection = \"macav2\"\n",
    "mongo_url = \"mongodb://lattice-100:27018/\"\n",
    "mongo_db_name = \"sustaindb\"\n",
    "query_fild = \"gis_join\"\n",
    "sample_percent = 0.1\n",
    "train_test = 0.8\n",
    "feature_importance_percentage = 98\n",
    "exhaustive_sample_percent = 0.0001\n",
    "\n",
    "\n",
    "training_labels = [\"min_surface_downwelling_shortwave_flux_in_air\", \"max_surface_downwelling_shortwave_flux_in_air\",\n",
    "                   \"max_specific_humidity\", \"min_max_air_temperature\", \"max_max_air_temperature\"]\n",
    "target_labels = [\"max_min_air_temperature\"]\n",
    "\n",
    "# QUERY-RELATED\n",
    "sustainclient = pymongo.MongoClient(mongo_url)\n",
    "sustain_db = sustainclient[mongo_db_name]\n",
    "\n",
    "# QUERY projection\n",
    "client_projection = {}\n",
    "for val in training_labels:\n",
    "    client_projection[val] = 1\n",
    "for val in target_labels:\n",
    "    client_projection[val] = 1\n",
    "    \n",
    "    \n",
    "def fancy_logging(msg, unique_id=\"\"):\n",
    "    print(unique_id, \":\", \"====================================\")\n",
    "    print(unique_id, \":\", msg, \": TIME: \",time())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-pharmacy",
   "metadata": {},
   "source": [
    "<h3><u>DATA FETCH</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "controlled-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL QUERYING\n",
    "def query_sustaindb(query_gisjoin):\n",
    "\n",
    "    sustain_collection = sustain_db[query_collection]\n",
    "    client_query = {query_fild: query_gisjoin}\n",
    "\n",
    "    start_time = time()\n",
    "    query_results = list(sustain_collection.find(client_query, client_projection))\n",
    "    \n",
    "    return list(query_results)\n",
    "\n",
    "def queryall_sustaindb():\n",
    "\n",
    "    sustain_collection = sustain_db[query_collection]\n",
    "    client_query = {}\n",
    "\n",
    "    start_time = time()\n",
    "    query_results = list(sustain_collection.find(client_query, client_projection))\n",
    "    \n",
    "    return list(query_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "metropolitan-assignment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  11353524\n"
     ]
    }
   ],
   "source": [
    "#df = query_sustaindb('G3701310')\n",
    "df = queryall_sustaindb()\n",
    "print(\"1: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-receptor",
   "metadata": {},
   "source": [
    "<h3><u>DATA SAMPLING</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "built-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sampling(query_results, exhaustive, sample_percent=1):\n",
    "    if exhaustive:\n",
    "        all_data = query_results\n",
    "    else:\n",
    "        data_size = int(len(query_results) * sample_percent)\n",
    "        all_data = sample(query_results, data_size)\n",
    "\n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mysterious-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIKI\n",
    "sampled_df = data_sampling(df, False, exhaustive_sample_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "liberal-trouble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1135, 5) (1135, 1)\n"
     ]
    }
   ],
   "source": [
    "Y = sampled_df.loc[:,target_labels]\n",
    "X = sampled_df.loc[:, training_labels]\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-parallel",
   "metadata": {},
   "source": [
    "<h3><u>DATA SPLITTING INTO TRAING AND VALIDATION</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "differential-porter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def data_partitioning(query_results, exhaustive, sample_percent=1):\\n    if exhaustive:\\n        all_data = query_results\\n    else:\\n        data_size = int(len(query_results) * sample_percent)\\n        all_data = sample(query_results, data_size)\\n\\n    msk = np.random.rand(len(all_data)) < train_test\\n\\n    all_data = pd.DataFrame(all_data)\\n    training_data = all_data[msk]\\n    val_data = all_data[~msk]\\n    return (pd.DataFrame(training_data), pd.DataFrame(val_data))'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def data_partitioning(query_results, exhaustive, sample_percent=1):\n",
    "    if exhaustive:\n",
    "        all_data = query_results\n",
    "    else:\n",
    "        data_size = int(len(query_results) * sample_percent)\n",
    "        all_data = sample(query_results, data_size)\n",
    "\n",
    "    msk = np.random.rand(len(all_data)) < train_test\n",
    "\n",
    "    all_data = pd.DataFrame(all_data)\n",
    "    training_data = all_data[msk]\n",
    "    val_data = all_data[~msk]\n",
    "    return (pd.DataFrame(training_data), pd.DataFrame(val_data))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "alien-geometry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(908, 5) (227, 5) (908, 1) (227, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-community",
   "metadata": {},
   "source": [
    "<h3><u>MODELING</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "turned-essex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parameters = [ {\\'n_estimators\\': 500, \\'max_depth\\': 2, \\'min_samples_split\\': 20},\\n                {\\'n_estimators\\': 300, \\'max_depth\\': 3, \\'min_samples_split\\': 50},\\n              {\\'n_estimators\\': 500, \\'max_depth\\': 3, \\'min_samples_split\\': 20},\\n              {\\'n_estimators\\': 600, \\'max_depth\\': 3, \\'min_samples_split\\': 15}]\\n    \\nfor params in parameters:\\n    print(\"PARAMETERS:\",params)\\n    count = 0\\n    error = 0\\n    for i in range(0,5):\\n        print(\"ROUND:\",i)\\n        clf = ensemble.RandomForestRegressor(**params)\\n        clf.fit(X_train, pd.Series.ravel(y_train))\\n\\n        rmse = sqrt(mean_squared_error(pd.Series.ravel(y_test), clf.predict(X_test)))\\n        print(rmse)\\n        error = error + rmse\\n\\n        feature_importance = clf.feature_importances_\\n        feature_importance = 100.0 * (feature_importance / feature_importance.sum())\\n        sorted_idx = np.argsort(feature_importance)\\n        count = count+1\\n        print(np.flip(sorted_idx), np.flip(feature_importance[sorted_idx]))\\n    \\n    print(\"===============================================================\")\\n\\n    print(\"AVG RMSE:\",(error/count))'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''parameters = [ {'n_estimators': 500, 'max_depth': 2, 'min_samples_split': 20},\n",
    "                {'n_estimators': 300, 'max_depth': 3, 'min_samples_split': 50},\n",
    "              {'n_estimators': 500, 'max_depth': 3, 'min_samples_split': 20},\n",
    "              {'n_estimators': 600, 'max_depth': 3, 'min_samples_split': 15}]\n",
    "    \n",
    "for params in parameters:\n",
    "    print(\"PARAMETERS:\",params)\n",
    "    count = 0\n",
    "    error = 0\n",
    "    for i in range(0,5):\n",
    "        print(\"ROUND:\",i)\n",
    "        clf = ensemble.RandomForestRegressor(**params)\n",
    "        clf.fit(X_train, pd.Series.ravel(y_train))\n",
    "\n",
    "        rmse = sqrt(mean_squared_error(pd.Series.ravel(y_test), clf.predict(X_test)))\n",
    "        print(rmse)\n",
    "        error = error + rmse\n",
    "\n",
    "        feature_importance = clf.feature_importances_\n",
    "        feature_importance = 100.0 * (feature_importance / feature_importance.sum())\n",
    "        sorted_idx = np.argsort(feature_importance)\n",
    "        count = count+1\n",
    "        print(np.flip(sorted_idx), np.flip(feature_importance[sorted_idx]))\n",
    "    \n",
    "    print(\"===============================================================\")\n",
    "\n",
    "    print(\"AVG RMSE:\",(error/count))'''\n",
    "\n",
    "# BETTER ALTERNATIVE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abstract-thousand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 150\n",
      "max_resources_: 600\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 6\n",
      "n_resources: 150\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV 1/5] END max_depth=2, min_samples_split=15, n_estimators=150;, score=(train=0.886, test=0.890) total time=   0.4s\n",
      "[CV 2/5] END max_depth=2, min_samples_split=15, n_estimators=150;, score=(train=0.879, test=0.886) total time=   0.3s\n",
      "[CV 3/5] END max_depth=2, min_samples_split=15, n_estimators=150;, score=(train=0.882, test=0.879) total time=   0.6s\n",
      "[CV 4/5] END max_depth=2, min_samples_split=15, n_estimators=150;, score=(train=0.885, test=0.891) total time=   0.3s\n",
      "[CV 5/5] END max_depth=2, min_samples_split=15, n_estimators=150;, score=(train=0.883, test=0.857) total time=   0.3s\n",
      "[CV 1/5] END max_depth=2, min_samples_split=20, n_estimators=150;, score=(train=0.886, test=0.890) total time=   0.3s\n",
      "[CV 2/5] END max_depth=2, min_samples_split=20, n_estimators=150;, score=(train=0.879, test=0.886) total time=   0.3s\n",
      "[CV 3/5] END max_depth=2, min_samples_split=20, n_estimators=150;, score=(train=0.882, test=0.879) total time=   0.3s\n",
      "[CV 4/5] END max_depth=2, min_samples_split=20, n_estimators=150;, score=(train=0.885, test=0.891) total time=   0.3s\n",
      "[CV 5/5] END max_depth=2, min_samples_split=20, n_estimators=150;, score=(train=0.883, test=0.857) total time=   0.3s\n",
      "[CV 1/5] END max_depth=2, min_samples_split=50, n_estimators=150;, score=(train=0.886, test=0.890) total time=   0.3s\n",
      "[CV 2/5] END max_depth=2, min_samples_split=50, n_estimators=150;, score=(train=0.879, test=0.886) total time=   0.3s\n",
      "[CV 3/5] END max_depth=2, min_samples_split=50, n_estimators=150;, score=(train=0.882, test=0.879) total time=   0.3s\n",
      "[CV 4/5] END max_depth=2, min_samples_split=50, n_estimators=150;, score=(train=0.885, test=0.891) total time=   0.4s\n",
      "[CV 5/5] END max_depth=2, min_samples_split=50, n_estimators=150;, score=(train=0.883, test=0.857) total time=   0.3s\n",
      "[CV 1/5] END max_depth=3, min_samples_split=15, n_estimators=150;, score=(train=0.938, test=0.937) total time=   0.4s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=15, n_estimators=150;, score=(train=0.932, test=0.932) total time=   0.4s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=15, n_estimators=150;, score=(train=0.936, test=0.934) total time=   0.3s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=15, n_estimators=150;, score=(train=0.937, test=0.932) total time=   0.3s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=15, n_estimators=150;, score=(train=0.934, test=0.915) total time=   0.4s\n",
      "[CV 1/5] END max_depth=3, min_samples_split=20, n_estimators=150;, score=(train=0.938, test=0.937) total time=   0.3s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=20, n_estimators=150;, score=(train=0.932, test=0.932) total time=   0.3s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=20, n_estimators=150;, score=(train=0.936, test=0.934) total time=   0.4s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=20, n_estimators=150;, score=(train=0.937, test=0.932) total time=   0.4s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=20, n_estimators=150;, score=(train=0.934, test=0.915) total time=   0.3s\n",
      "[CV 1/5] END max_depth=3, min_samples_split=50, n_estimators=150;, score=(train=0.938, test=0.937) total time=   0.5s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=50, n_estimators=150;, score=(train=0.932, test=0.932) total time=   0.4s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=50, n_estimators=150;, score=(train=0.936, test=0.934) total time=   0.3s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=50, n_estimators=150;, score=(train=0.937, test=0.932) total time=   0.5s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=50, n_estimators=150;, score=(train=0.934, test=0.915) total time=   0.4s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 3\n",
      "n_resources: 300\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV 1/5] END max_depth=3, min_samples_split=50, n_estimators=300;, score=(train=0.939, test=0.937) total time=   0.6s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=50, n_estimators=300;, score=(train=0.932, test=0.933) total time=   0.6s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=50, n_estimators=300;, score=(train=0.936, test=0.934) total time=   0.6s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=50, n_estimators=300;, score=(train=0.937, test=0.932) total time=   0.6s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=50, n_estimators=300;, score=(train=0.935, test=0.915) total time=   0.6s\n",
      "[CV 1/5] END max_depth=3, min_samples_split=15, n_estimators=300;, score=(train=0.939, test=0.937) total time=   0.6s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=15, n_estimators=300;, score=(train=0.932, test=0.933) total time=   0.6s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=15, n_estimators=300;, score=(train=0.936, test=0.934) total time=   0.7s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=15, n_estimators=300;, score=(train=0.937, test=0.932) total time=   0.6s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=15, n_estimators=300;, score=(train=0.935, test=0.915) total time=   0.8s\n",
      "[CV 1/5] END max_depth=3, min_samples_split=20, n_estimators=300;, score=(train=0.939, test=0.937) total time=   0.6s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=20, n_estimators=300;, score=(train=0.932, test=0.933) total time=   0.6s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=20, n_estimators=300;, score=(train=0.936, test=0.934) total time=   0.6s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=20, n_estimators=300;, score=(train=0.937, test=0.932) total time=   0.6s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=20, n_estimators=300;, score=(train=0.935, test=0.915) total time=   0.6s\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 600\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END max_depth=3, min_samples_split=15, n_estimators=600;, score=(train=0.939, test=0.936) total time=   1.2s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=15, n_estimators=600;, score=(train=0.933, test=0.933) total time=   1.2s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=15, n_estimators=600;, score=(train=0.936, test=0.934) total time=   1.2s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=15, n_estimators=600;, score=(train=0.937, test=0.932) total time=   1.2s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=15, n_estimators=600;, score=(train=0.935, test=0.915) total time=   1.2s\n",
      "[CV 1/5] END max_depth=3, min_samples_split=20, n_estimators=600;, score=(train=0.939, test=0.936) total time=   1.2s\n",
      "[CV 2/5] END max_depth=3, min_samples_split=20, n_estimators=600;, score=(train=0.933, test=0.933) total time=   1.4s\n",
      "[CV 3/5] END max_depth=3, min_samples_split=20, n_estimators=600;, score=(train=0.936, test=0.934) total time=   1.3s\n",
      "[CV 4/5] END max_depth=3, min_samples_split=20, n_estimators=600;, score=(train=0.937, test=0.932) total time=   1.2s\n",
      "[CV 5/5] END max_depth=3, min_samples_split=20, n_estimators=600;, score=(train=0.935, test=0.915) total time=   1.2s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "param_grid = {'max_depth': [2, 3], 'min_samples_split': [15, 20, 50]}\n",
    "base_est = ensemble.RandomForestRegressor(random_state=0)\n",
    "sh = HalvingGridSearchCV(base_est, param_grid, cv=5, verbose=3, \n",
    "                         factor=2, resource='n_estimators', max_resources=600).fit(X, pd.Series.ravel(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "agricultural-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE BEST MODEL\n",
    "clf_best = sh.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "settled-warning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8800691178184676"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = sqrt(mean_squared_error(pd.Series.ravel(y_test), clf_best.predict(X_test)))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-sarah",
   "metadata": {},
   "source": [
    "<h3><u>EXTRACT TOP FEATURES</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "diagnostic-device",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 3 1 0] [9.48530779e+01 4.81346793e+00 3.32294507e-01 1.15970774e-03\n",
      " 0.00000000e+00]\n",
      "[2 4 3 1 0]\n",
      "[9.48530779e+01 4.81346793e+00 3.32294507e-01 1.15970774e-03\n",
      " 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "feature_importance = clf_best.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.sum())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "print(np.flip(sorted_idx), np.flip(feature_importance[sorted_idx]))\n",
    "\n",
    "feature_importance = np.flip(feature_importance[sorted_idx])\n",
    "sorted_idx=np.flip(sorted_idx)\n",
    "\n",
    "print(sorted_idx)\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-israeli",
   "metadata": {},
   "source": [
    "<h3><u>FIND N FOR WHICH IMPORTANCE % > feature-importance-percentage</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "apparent-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cumulative(lists, val_max):\n",
    "    cu_list = []\n",
    "    length = len(lists)\n",
    "    cu_list = [sum(lists[0:x:1]) for x in range(1, length+1)]\n",
    "    \n",
    "    print(cu_list)\n",
    "    res = next(x for x, val in enumerate(cu_list)\n",
    "                                  if val > val_max)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "convenient-footage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94.85307785260474, 99.66654578540808, 99.99884029225913, 99.99999999999999, 99.99999999999999]\n",
      "LAST INDEX:  1\n"
     ]
    }
   ],
   "source": [
    "cut_off_indx = find_cumulative(feature_importance, feature_importance_percentage)\n",
    "\n",
    "print(\"LAST INDEX: \", cut_off_indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "third-handling",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 3 1 0]\n",
      "[2 4]\n"
     ]
    }
   ],
   "source": [
    "chopped_indices = sorted_idx[0:cut_off_indx+1]\n",
    "\n",
    "print(sorted_idx)\n",
    "print(chopped_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-profit",
   "metadata": {},
   "source": [
    "<h3><u>SELECTED TOP COLUMNS</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "expressed-motor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['min_surface_downwelling_shortwave_flux_in_air', 'max_surface_downwelling_shortwave_flux_in_air', 'max_specific_humidity', 'min_max_air_temperature', 'max_max_air_temperature']\n",
      "['max_min_air_temperature']\n"
     ]
    }
   ],
   "source": [
    "candidate_x_columns = list(X.columns)\n",
    "candidate_y_columns = list(Y.columns)\n",
    "\n",
    "print(candidate_x_columns)\n",
    "print(candidate_y_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "political-sight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['max_specific_humidity', 'max_max_air_temperature']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_x_columns = [candidate_x_columns[i] for i in chopped_indices]\n",
    "selected_x_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-topic",
   "metadata": {},
   "source": [
    "<b><hr /></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-strand",
   "metadata": {},
   "source": [
    "<h1><u><b>TRAINING PHASE #2</b></u></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-boston",
   "metadata": {},
   "source": [
    "<h3><u>AGGREGATE QUERY OVER THE CHOSEN COLUMNS PER GIS-JOIN</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fitting-nylon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sustain_collection = sustain_db[query_collection]\\npipeline=[\\n   { \"$project\": { \\'gis_join\\': \\'$gis_join\\', \\'max_specific_humidity\\': \\'$max_specific_humidity\\', \\'max_max_air_temperature\\': \\'$max_max_air_temperature\\', \\'max_min_air_temperature\\': \\'$max_min_air_temperature\\'}},\\n   { \"$group\": { \\'_id\\': \"$gis_join\", \\n\"avg_max_specific_humidity\": { \"$avg\": \"$max_specific_humidity\" },\\n\"avg_max_max_air_temperature\": { \"$avg\": \"$max_max_air_temperature\" },\\n\"avg_max_min_air_temperature\": { \"$avg\": \"$max_min_air_temperature\" }\\n  } }\\n]\\ncur = sustain_collection.aggregate(pipeline)\\n\\nresults = list(cur)\\nlen(results)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''sustain_collection = sustain_db[query_collection]\n",
    "pipeline=[\n",
    "   { \"$project\": { 'gis_join': '$gis_join', 'max_specific_humidity': '$max_specific_humidity', 'max_max_air_temperature': '$max_max_air_temperature', 'max_min_air_temperature': '$max_min_air_temperature'}},\n",
    "   { \"$group\": { '_id': \"$gis_join\", \n",
    "\"avg_max_specific_humidity\": { \"$avg\": \"$max_specific_humidity\" },\n",
    "\"avg_max_max_air_temperature\": { \"$avg\": \"$max_max_air_temperature\" },\n",
    "\"avg_max_min_air_temperature\": { \"$avg\": \"$max_min_air_temperature\" }\n",
    "  } }\n",
    "]\n",
    "cur = sustain_collection.aggregate(pipeline)\n",
    "\n",
    "results = list(cur)\n",
    "len(results)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "known-situation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['max_specific_humidity', 'max_max_air_temperature', 'max_min_air_temperature']\n"
     ]
    }
   ],
   "source": [
    "chopped_projection = []\n",
    "chopped_projection.extend(selected_x_columns)\n",
    "chopped_projection.extend(candidate_y_columns)\n",
    "\n",
    "print(chopped_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "happy-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_chopped_query(chopped_projection, gis_join):\n",
    "    # PROJECTION\n",
    "    proj_d = {}\n",
    "    proj_dict = {'$project': proj_d}\n",
    "    \n",
    "    #GROUP + AGGREGATION\n",
    "    group_d = {}\n",
    "    group_dict = {'$group': group_d}\n",
    "    \n",
    "    full_query=[proj_dict, group_dict]\n",
    "    \n",
    "    # PROJECTION PART\n",
    "    for cp in chopped_projection:\n",
    "        proj_d[cp] = \"$\"+str(cp)\n",
    "    proj_d[gis_join] = \"$\"+str(gis_join)\n",
    "    \n",
    "    # GROUP PART\n",
    "    group_d['_id'] = \"$\"+str(gis_join)\n",
    "    for cp in chopped_projection:\n",
    "        inner_dict = {}\n",
    "        inner_dict[\"$avg\"] = \"$\"+str(cp)\n",
    "        group_d[cp] = inner_dict\n",
    "    \n",
    "    return full_query\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "inclusive-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_pipeline = construct_chopped_query(chopped_projection, query_fild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "recreational-visiting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3108\n"
     ]
    }
   ],
   "source": [
    "sustain_collection = sustain_db[query_collection]\n",
    "cur = sustain_collection.aggregate(agg_pipeline)\n",
    "agg_results = list(cur)\n",
    "\n",
    "print(len(agg_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "promising-rocket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': 'G2101030',\n",
       " 'max_specific_humidity': 0.00779030933479332,\n",
       " 'max_max_air_temperature': 292.74565316178484,\n",
       " 'max_min_air_temperature': 280.454}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-water",
   "metadata": {},
   "source": [
    "<h3><u>DATA STAGING FOR PHASE 2</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "advisory-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase2_df = pd.DataFrame(agg_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "suburban-bidding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['max_specific_humidity', 'max_max_air_temperature', 'max_min_air_temperature']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chopped_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "finnish-spank",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_specific_humidity</th>\n",
       "      <th>max_max_air_temperature</th>\n",
       "      <th>max_min_air_temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007790</td>\n",
       "      <td>292.745653</td>\n",
       "      <td>280.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009356</td>\n",
       "      <td>297.660267</td>\n",
       "      <td>284.548030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.009270</td>\n",
       "      <td>296.434550</td>\n",
       "      <td>283.163267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009094</td>\n",
       "      <td>297.060173</td>\n",
       "      <td>284.019307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008782</td>\n",
       "      <td>295.071615</td>\n",
       "      <td>282.488024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3103</th>\n",
       "      <td>0.009323</td>\n",
       "      <td>296.016510</td>\n",
       "      <td>285.450908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3104</th>\n",
       "      <td>0.007281</td>\n",
       "      <td>297.902625</td>\n",
       "      <td>282.524589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105</th>\n",
       "      <td>0.008150</td>\n",
       "      <td>293.403102</td>\n",
       "      <td>281.550927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3106</th>\n",
       "      <td>0.010553</td>\n",
       "      <td>298.749899</td>\n",
       "      <td>285.791208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>0.007482</td>\n",
       "      <td>292.986748</td>\n",
       "      <td>280.239136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3108 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      max_specific_humidity  max_max_air_temperature  max_min_air_temperature\n",
       "0                  0.007790               292.745653               280.454000\n",
       "1                  0.009356               297.660267               284.548030\n",
       "2                  0.009270               296.434550               283.163267\n",
       "3                  0.009094               297.060173               284.019307\n",
       "4                  0.008782               295.071615               282.488024\n",
       "...                     ...                      ...                      ...\n",
       "3103               0.009323               296.016510               285.450908\n",
       "3104               0.007281               297.902625               282.524589\n",
       "3105               0.008150               293.403102               281.550927\n",
       "3106               0.010553               298.749899               285.791208\n",
       "3107               0.007482               292.986748               280.239136\n",
       "\n",
       "[3108 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_importance = phase2_df.loc[:, chopped_projection]\n",
    "df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-bullet",
   "metadata": {},
   "source": [
    "<h3><u>K-MEANS CLUSTERING</u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bridal-cartridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = int(sqrt(len(agg_results)))\n",
    "\n",
    "num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "superior-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "copyrighted-behavior",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-3.40282347e+38 -3.40282347e+38 -3.40282347e+38]\n",
      " [-7.37869763e+19 -7.37869763e+19 -7.37869763e+19]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3072352/45823521.py:1: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (55). Possibly due to duplicate points in X.\n",
      "  kmeans = KMeans(n_clusters=num_clusters).fit(df_importance)\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=num_clusters).fit(df_importance)\n",
    "centroids = kmeans.cluster_centers_\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "comparable-young",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      gis_join cluster_id    distance\n",
      "0     G2101030          0  405.406541\n",
      "1     G4500330          0  411.787829\n",
      "2     G1301150          0  409.944970\n",
      "3     G3700070          0  410.988702\n",
      "4     G4701890          0  408.493257\n",
      "...        ...        ...         ...\n",
      "3103  G3700290          0  411.227425\n",
      "3104  G4800450          0  410.568042\n",
      "3105  G1801230          0  406.640265\n",
      "3106  G2200810          0  413.434538\n",
      "3107  G3900870          0  405.432124\n",
      "\n",
      "[3108 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "df_ultimate = pd.DataFrame(columns=[\"gis_join\", \"cluster_id\", \"distance\"])\n",
    "\n",
    "\n",
    "\n",
    "for index, row in phase2_df.iterrows():\n",
    "    input_x = row[chopped_projection]\n",
    "    gis_join = row['_id']\n",
    "    #print(input_x, gis_join)\n",
    "    closest, d = pairwise_distances_argmin_min([np.array(input_x)], centroids)\n",
    "    df_ultimate.loc[index] = [gis_join, closest[0], d[0]]\n",
    "    \n",
    "print(df_ultimate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "likely-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultimate.to_csv(\"/tmp/clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-partnership",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
